{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfWxNSk4Iic3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.optimizer import Optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcc-SkuI54T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1000\n",
        "EPOCHS_LOG_INTERVAL = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuCErCteKQez",
        "colab_type": "code",
        "outputId": "af019439-3b96-4072-efbd-a68c22f1eab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "print('Example data: ')\n",
        "print(iris.data[:5])\n",
        "print('Example labels: ')\n",
        "print(iris.target[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example data: \n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "Example labels: \n",
            "[0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MpequryM59U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # define nn\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = self.fc2(X)\n",
        "        X = self.fc3(X)\n",
        "        X = self.softmax(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data for training and testing\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "train_X = Variable(torch.Tensor(train_x).float())\n",
        "test_X = Variable(torch.Tensor(test_x).float())\n",
        "train_y = Variable(torch.Tensor(train_y).long())\n",
        "test_y = Variable(torch.Tensor(test_y).long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgJBl8pdYZMu",
        "colab_type": "code",
        "outputId": "5599bfb2-01c9-456f-b34d-4771cd1411ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(train_X)\n",
        "    loss = criterion(out, train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % EPOCHS_LOG_INTERVAL == 0:\n",
        "        print ('Epoch #', epoch, 'loss: ', loss.data.item())\n",
        "\n",
        "predict_out = model(test_X)\n",
        "_, predict_y = torch.max(predict_out, 1)\n",
        "print ('prediction accuracy', accuracy_score(test_y.data, predict_y.data))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch # 0 loss:  1.1231729984283447\n",
            "Epoch # 10 loss:  1.0845417976379395\n",
            "Epoch # 20 loss:  1.061902642250061\n",
            "Epoch # 30 loss:  1.040486216545105\n",
            "Epoch # 40 loss:  1.017595648765564\n",
            "Epoch # 50 loss:  0.9945967793464661\n",
            "Epoch # 60 loss:  0.9729045629501343\n",
            "Epoch # 70 loss:  0.9532913565635681\n",
            "Epoch # 80 loss:  0.9359519481658936\n",
            "Epoch # 90 loss:  0.9207199811935425\n",
            "Epoch # 100 loss:  0.9072737097740173\n",
            "Epoch # 110 loss:  0.8952866196632385\n",
            "Epoch # 120 loss:  0.8844863176345825\n",
            "Epoch # 130 loss:  0.8746868371963501\n",
            "Epoch # 140 loss:  0.865768551826477\n",
            "Epoch # 150 loss:  0.8576305508613586\n",
            "Epoch # 160 loss:  0.8501619696617126\n",
            "Epoch # 170 loss:  0.8432596325874329\n",
            "Epoch # 180 loss:  0.8368533849716187\n",
            "Epoch # 190 loss:  0.8308594822883606\n",
            "Epoch # 200 loss:  0.8252092003822327\n",
            "Epoch # 210 loss:  0.8198577165603638\n",
            "Epoch # 220 loss:  0.8147523999214172\n",
            "Epoch # 230 loss:  0.8098625540733337\n",
            "Epoch # 240 loss:  0.8051660656929016\n",
            "Epoch # 250 loss:  0.8006303906440735\n",
            "Epoch # 260 loss:  0.7962320446968079\n",
            "Epoch # 270 loss:  0.7919518351554871\n",
            "Epoch # 280 loss:  0.7877732515335083\n",
            "Epoch # 290 loss:  0.7836812138557434\n",
            "Epoch # 300 loss:  0.779669463634491\n",
            "Epoch # 310 loss:  0.7757313847541809\n",
            "Epoch # 320 loss:  0.7718560099601746\n",
            "Epoch # 330 loss:  0.7680407762527466\n",
            "Epoch # 340 loss:  0.764293909072876\n",
            "Epoch # 350 loss:  0.7606002688407898\n",
            "Epoch # 360 loss:  0.7569679617881775\n",
            "Epoch # 370 loss:  0.7533878087997437\n",
            "Epoch # 380 loss:  0.7498605251312256\n",
            "Epoch # 390 loss:  0.7463840842247009\n",
            "Epoch # 400 loss:  0.7429563403129578\n",
            "Epoch # 410 loss:  0.7395792603492737\n",
            "Epoch # 420 loss:  0.7362548112869263\n",
            "Epoch # 430 loss:  0.7329809665679932\n",
            "Epoch # 440 loss:  0.7297618389129639\n",
            "Epoch # 450 loss:  0.7265981435775757\n",
            "Epoch # 460 loss:  0.7234880328178406\n",
            "Epoch # 470 loss:  0.7204321622848511\n",
            "Epoch # 480 loss:  0.71743243932724\n",
            "Epoch # 490 loss:  0.7144911885261536\n",
            "Epoch # 500 loss:  0.7116076350212097\n",
            "Epoch # 510 loss:  0.7087821364402771\n",
            "Epoch # 520 loss:  0.7060161828994751\n",
            "Epoch # 530 loss:  0.7033112049102783\n",
            "Epoch # 540 loss:  0.7006673216819763\n",
            "Epoch # 550 loss:  0.6980847716331482\n",
            "Epoch # 560 loss:  0.6955628991127014\n",
            "Epoch # 570 loss:  0.6931005716323853\n",
            "Epoch # 580 loss:  0.6906967163085938\n",
            "Epoch # 590 loss:  0.6883529424667358\n",
            "Epoch # 600 loss:  0.686067521572113\n",
            "Epoch # 610 loss:  0.6838409304618835\n",
            "Epoch # 620 loss:  0.6816720962524414\n",
            "Epoch # 630 loss:  0.6795599460601807\n",
            "Epoch # 640 loss:  0.6775037050247192\n",
            "Epoch # 650 loss:  0.6755017042160034\n",
            "Epoch # 660 loss:  0.6735541224479675\n",
            "Epoch # 670 loss:  0.6716590523719788\n",
            "Epoch # 680 loss:  0.6698159575462341\n",
            "Epoch # 690 loss:  0.6680231690406799\n",
            "Epoch # 700 loss:  0.6662806272506714\n",
            "Epoch # 710 loss:  0.6645873188972473\n",
            "Epoch # 720 loss:  0.6629419922828674\n",
            "Epoch # 730 loss:  0.6613420248031616\n",
            "Epoch # 740 loss:  0.6597866415977478\n",
            "Epoch # 750 loss:  0.6582740545272827\n",
            "Epoch # 760 loss:  0.6568030118942261\n",
            "Epoch # 770 loss:  0.6553731560707092\n",
            "Epoch # 780 loss:  0.6539823412895203\n",
            "Epoch # 790 loss:  0.6526307463645935\n",
            "Epoch # 800 loss:  0.6513158679008484\n",
            "Epoch # 810 loss:  0.6500378251075745\n",
            "Epoch # 820 loss:  0.6487953066825867\n",
            "Epoch # 830 loss:  0.6475868821144104\n",
            "Epoch # 840 loss:  0.646411120891571\n",
            "Epoch # 850 loss:  0.6452661156654358\n",
            "Epoch # 860 loss:  0.6441485285758972\n",
            "Epoch # 870 loss:  0.6430594325065613\n",
            "Epoch # 880 loss:  0.641999363899231\n",
            "Epoch # 890 loss:  0.6409651637077332\n",
            "Epoch # 900 loss:  0.6399564146995544\n",
            "Epoch # 910 loss:  0.638972282409668\n",
            "Epoch # 920 loss:  0.6380085349082947\n",
            "Epoch # 930 loss:  0.6370728015899658\n",
            "Epoch # 940 loss:  0.6361647844314575\n",
            "Epoch # 950 loss:  0.6352800726890564\n",
            "Epoch # 960 loss:  0.6344206929206848\n",
            "Epoch # 970 loss:  0.6335833668708801\n",
            "Epoch # 980 loss:  0.6327663660049438\n",
            "Epoch # 990 loss:  0.6319699883460999\n",
            "prediction accuracy 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFbGPkaG6pCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ed5ddfd-136a-4d2e-e215-ba47900df67f"
      },
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class SimulatedAnnealing(Optimizer):\n",
        "    def __init__(self, params, t0 = 1.0\n",
        "                 , anneal_rate = 0.0003, neighborhoodSize = 0.05\n",
        "                 , loss = nn.CrossEntropyLoss()\n",
        "                 , model = None\n",
        "                 , features = None\n",
        "                 , labels = None): #these represent default values, but can be overridden\n",
        "        self.t0 = t0\n",
        "        self.anneal_rate = anneal_rate\n",
        "        self.t = t0\n",
        "        self.loss = loss\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.neighborhoodSize = neighborhoodSize\n",
        "        self.iterations = 1\n",
        "        self.min_t = 1e-5\n",
        "\n",
        "    def step(self):\n",
        "        old_loss = self.loss(self.model(self.features), self.labels.type(torch.LongTensor))\n",
        "        old_state_dict = {}\n",
        "        for key in self.model.state_dict():\n",
        "            old_state_dict[key] = self.model.state_dict()[key].clone()\n",
        "\n",
        "        for name, param in self.model.state_dict().items():\n",
        "            if (len(param.shape) == 2):\n",
        "                random = torch.Tensor(np.random.uniform(low = self.neighborhoodSize * -1, high = self.neighborhoodSize\n",
        "                                                        , size = (param.shape[0], param.shape[1])))\n",
        "            else:\n",
        "                random = torch.Tensor(np.random.uniform(low = self.neighborhoodSize * -1, high = self.neighborhoodSize\n",
        "                                                        , size = param.shape[0]))\n",
        "                                                        \n",
        "            new_param = param + random\n",
        "            self.model.state_dict()[name].copy_(new_param)\n",
        "            self.t = max(self.t0 * self.anneal_rate / self.iterations, self.min_t)\n",
        "            self.iterations += 1\n",
        "\n",
        "        newPerformance = self.loss(self.model(self.features), self.labels.type(torch.LongTensor))\n",
        "        if (newPerformance > old_loss):\n",
        "            jumpProb = torch.exp((newPerformance - old_loss) / self.t) \n",
        "            if (np.random.uniform(0, 1) > jumpProb): \n",
        "                self.model.load_state_dict(old_state_dict)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = SimulatedAnnealing(\n",
        "    params = model.parameters(), model = model, features = train_X, labels = train_y\n",
        ")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  output = model(train_X)\n",
        "  loss = criterion(output, train_y)\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    print ('number of epoch', epoch, ' loss ', loss.data.item())\n",
        "\n",
        "predict_out = model(test_X)\n",
        "_, predict_y = torch.max(predict_out, 1)\n",
        "print ('prediction accuracy', accuracy_score(test_y.data, predict_y.data))\n",
        "\n",
        "  "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epoch 0  loss  1.1110694408416748\n",
            "number of epoch 100  loss  1.2180982828140259\n",
            "number of epoch 200  loss  1.2347773313522339\n",
            "number of epoch 300  loss  1.2070424556732178\n",
            "number of epoch 400  loss  1.2181106805801392\n",
            "number of epoch 500  loss  1.2181106805801392\n",
            "number of epoch 600  loss  1.2347760200500488\n",
            "number of epoch 700  loss  1.2401587963104248\n",
            "number of epoch 800  loss  1.2181106805801392\n",
            "number of epoch 900  loss  1.2014440298080444\n",
            "number of epoch 1000  loss  1.2014440298080444\n",
            "number of epoch 1100  loss  1.2014440298080444\n",
            "number of epoch 1200  loss  1.2014440298080444\n",
            "number of epoch 1300  loss  1.2014440298080444\n",
            "number of epoch 1400  loss  1.2014440298080444\n",
            "number of epoch 1500  loss  1.2014440298080444\n",
            "number of epoch 1600  loss  1.2014440298080444\n",
            "number of epoch 1700  loss  1.2014440298080444\n",
            "number of epoch 1800  loss  1.2014440298080444\n",
            "number of epoch 1900  loss  1.2014440298080444\n",
            "number of epoch 2000  loss  1.2014440298080444\n",
            "number of epoch 2100  loss  1.2014440298080444\n",
            "number of epoch 2200  loss  1.2014440298080444\n",
            "number of epoch 2300  loss  1.2014440298080444\n",
            "number of epoch 2400  loss  1.2014440298080444\n",
            "number of epoch 2500  loss  1.2014440298080444\n",
            "number of epoch 2600  loss  1.2014440298080444\n",
            "number of epoch 2700  loss  1.2014440298080444\n",
            "number of epoch 2800  loss  1.5431092977523804\n",
            "number of epoch 2900  loss  1.2031852006912231\n",
            "number of epoch 3000  loss  1.2014440298080444\n",
            "number of epoch 3100  loss  1.5261921882629395\n",
            "number of epoch 3200  loss  1.2014440298080444\n",
            "number of epoch 3300  loss  1.534775972366333\n",
            "number of epoch 3400  loss  1.2014440298080444\n",
            "number of epoch 3500  loss  1.2014440298080444\n",
            "number of epoch 3600  loss  1.2097773551940918\n",
            "number of epoch 3700  loss  1.2181106805801392\n",
            "number of epoch 3800  loss  1.2181106805801392\n",
            "number of epoch 3900  loss  1.2347773313522339\n",
            "number of epoch 4000  loss  1.2347773313522339\n",
            "number of epoch 4100  loss  1.2347773313522339\n",
            "number of epoch 4200  loss  1.2347773313522339\n",
            "number of epoch 4300  loss  1.2347773313522339\n",
            "number of epoch 4400  loss  1.2347773313522339\n",
            "number of epoch 4500  loss  1.2347773313522339\n",
            "number of epoch 4600  loss  1.2347773313522339\n",
            "number of epoch 4700  loss  0.8847782611846924\n",
            "number of epoch 4800  loss  0.8847782611846924\n",
            "number of epoch 4900  loss  0.8847782611846924\n",
            "number of epoch 5000  loss  1.1886509656906128\n",
            "number of epoch 5100  loss  1.2347773313522339\n",
            "number of epoch 5200  loss  0.9014449715614319\n",
            "number of epoch 5300  loss  0.8847782611846924\n",
            "number of epoch 5400  loss  1.2264440059661865\n",
            "number of epoch 5500  loss  1.2347773313522339\n",
            "number of epoch 5600  loss  1.2264440059661865\n",
            "number of epoch 5700  loss  1.2347773313522339\n",
            "number of epoch 5800  loss  1.159777045249939\n",
            "number of epoch 5900  loss  1.2347773313522339\n",
            "number of epoch 6000  loss  0.9014449715614319\n",
            "number of epoch 6100  loss  1.2347773313522339\n",
            "number of epoch 6200  loss  1.0264461040496826\n",
            "number of epoch 6300  loss  0.9850819110870361\n",
            "number of epoch 6400  loss  1.2181106805801392\n",
            "number of epoch 6500  loss  1.2095787525177002\n",
            "number of epoch 6600  loss  1.1514443159103394\n",
            "number of epoch 6700  loss  1.2347773313522339\n",
            "number of epoch 6800  loss  1.2347773313522339\n",
            "number of epoch 6900  loss  1.2347773313522339\n",
            "number of epoch 7000  loss  1.1514110565185547\n",
            "number of epoch 7100  loss  1.2347773313522339\n",
            "number of epoch 7200  loss  1.2347773313522339\n",
            "number of epoch 7300  loss  1.2347773313522339\n",
            "number of epoch 7400  loss  1.2347773313522339\n",
            "number of epoch 7500  loss  1.2347773313522339\n",
            "number of epoch 7600  loss  1.2347773313522339\n",
            "number of epoch 7700  loss  1.1585087776184082\n",
            "number of epoch 7800  loss  1.2347773313522339\n",
            "number of epoch 7900  loss  0.9014449715614319\n",
            "number of epoch 8000  loss  0.9597770571708679\n",
            "number of epoch 8100  loss  1.2347773313522339\n",
            "number of epoch 8200  loss  1.2347773313522339\n",
            "number of epoch 8300  loss  1.2347773313522339\n",
            "number of epoch 8400  loss  1.1764415502548218\n",
            "number of epoch 8500  loss  1.309776782989502\n",
            "number of epoch 8600  loss  1.2347773313522339\n",
            "number of epoch 8700  loss  1.334776759147644\n",
            "number of epoch 8800  loss  1.2426480054855347\n",
            "number of epoch 8900  loss  1.3597043752670288\n",
            "number of epoch 9000  loss  1.2347772121429443\n",
            "number of epoch 9100  loss  1.2347773313522339\n",
            "number of epoch 9200  loss  1.2347773313522339\n",
            "number of epoch 9300  loss  1.1931114196777344\n",
            "number of epoch 9400  loss  1.5014426708221436\n",
            "number of epoch 9500  loss  1.534775972366333\n",
            "number of epoch 9600  loss  1.5431092977523804\n",
            "number of epoch 9700  loss  1.3847764730453491\n",
            "number of epoch 9800  loss  1.2264158725738525\n",
            "number of epoch 9900  loss  1.3348402976989746\n",
            "prediction accuracy 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}